\documentclass{article}

\usepackage{amssymb, amsthm, amstext, amsxtra, amsmath, amsfonts, amscd }
\usepackage{graphicx, epsfig, apacite, dirtytalk, fancyhdr}
\usepackage{comment, color, cleveref, listings, wrapfig}
\usepackage{pdflscape}
\usepackage[left=3cm, right=3cm, bottom=3cm, top=3cm]{geometry}


\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Informatics Research Proposal}
\fancyhead[R]{ \leftmark}
\fancyfoot[C]{\thepage}

\def\E{\mathbb{E}}
\def\pr{\mathbb{P}}
\renewcommand\baselinestretch{1.4}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\corr}{corr}

\author{Salvador Garcia, s1655274 \\
s1655274@sms.ed.ac.uk}
\date{April 14th 2017}
\title{Developing a software for analyzing data from online learning quizzes.}

\begin{document}

\maketitle

\section*{Introduction}
In educational literature different methodologies had been developed to assess student's knowledge and comprehension of a topic. One of them consists in the measurement of the performance of a student through high-stakes and low-stakes tests. While the first does not give a stimulus for a constant preparation of the provided material, the second requires that the student study regularly the class material. These frequent-basis tests can provide relevant information about the student understanding of a topic. Furthermore, if these are applied with an online learning tool, extra information can be generated to monitor student's learning. This project consists in the development of an \textsf{R}-package and an user interface based in the software package \textit{lanalytics}, which make use of the information generated by online learning tools and give analysis about the test performance of the students.

% ===========================================
  % ===========================================
  % ===========================================
  
  \section*{Background} 
%Background: a short description of how previous work addresses (or fails to address) this problem, leading to a rationale for the hypotheses that you intend to test, and a convincing argument about how that hypotheses might solve the problem.

The high-stakes tests are those that have a huge reward or penalty for the person that takes it. In the context of a class assessment, these are not frequent test; in consequence, the student and the professor have no way to assess the correct knowledge and comprehension of the evaluated topics. On the other hand, the high frequency low-stakes tests can benefit the class-taker and the class-lecturer with a constant feedback for both parts. Some particular examples are the individual and the team Readiness Assurance Tests (iRAT and tRAT respectively) proposed in the team-based learning sequence by \cite{michaelsen2002team}.
The iRAT and tRAT are frequent tests that can measure the knowledge, understanding and comprehension of concepts and topics before each class. Although these test can be applied separately, the \textit{team-based learning sequence} \cite{michaelsen2002team} propose that the individual RATs should be made first by students, after that, the team RAT should be made to compare answers and receive feedback from their schoolfellows. The educators should determine the level of preparation required for each class and also the rewards given by the tests. With the obtained answers from students, the professor can be aware of misconceptions and difficult topics for the students; in consequence, he can adapt the class to address this problems. This method is called Just in Time Teaching (JiTT) \cite{jitt}. 

By analyzing the results of the frequent-basis low-stakes tests and by performing Just in Time Teaching, the educators can obtain insights about the general and the specific performance of the group with respect to each topic. Then they can take a decision about the distribution of the time in the class, choosing between topics that are well understood and topics where there is a general lack of knowledge or understanding. With this relevant information, the educator can try to maximize the student's learning in the constrained amount of class time. 

Currently, there are tools that facilitates the monitoring of the answers in low-stakes tests \footnote{For example, the learning catalytics software \cite{Schell2013}.}, but there is a lack of open-source analysis tools. To address the situation, a software package called \textit{lanalytics} was created by Melanie I. Stefan \cite{Stefan2015b}. This package performs relevant analysis using the answers given by students and the timestamps of each one, as well as the item easiness (as the ratio between correct and total number of questions) and the item cognitive level (measured by the professors into three categories: factual knowledge or terminology, understanding of concept, comprehension of concept). In addition, some more general ideas had been proposed, as the \textit{Open Learning Analytics} that is a platform that consist in a dashboard and engines that perform analysis about students, researchers, educators and institutions \cite{siemens2011open}.

% ===========================================
% ===========================================
% ===========================================

\section*{Purpose} 

%Purpose: a statement of the problem to be addressed. This should include arguments as to why solving the problem is important; e.g., because it will enable certain applications, or lead to interesting scientific discoveries.

The purpose of this project is to publish an open source analysis \textsf{R}-package and an interface based on the \textit{lanalytics} software package and enhance it with more detailed analysis. This can help to improve the item and test quality for each course and compare different years results. The package and interface could be used by a large community of people that are interested in low-stake tests and the associated benefits of it.

% ===========================================
% ===========================================
% ===========================================

\section*{Methods}
%Methods: A description of the methods and techniques you intend to use to test your hypotheses (e.g., data analysis procedures, experimental design etc), indicating that alternatives have been considered and ruled out on sound scientific grounds.

The \textsf{R}-package and user interface that will be created should accomplish two main objectives. The first is that it should sufficiently versatile for \textsf{R}-users and \textsf{R}developers, while the second is that it should be sufficiently user-friendly for people with no background in programming. 

To make an \textsf{R}-package we should be sure that the code is reproducible. To do that, R has to follow a package structure and some standards related to the metadata, the namespaces, the vignettes, the documentation and the sample data. These rules help to make a package reproducible, user-friendly and set the basics for future developments. In this section a brief description of these standards will be given \cite{wickham2015r}:

\begin{itemize}
\item Metadata (\texttt{DESCRIPTION}) : Is a general description file, including package name, author, version, package imports (and versions), package dependencies (and versions), license and description.
\item Namespaces (\texttt{NAMESPACE}): In general, the specification of this file is to avoid conflicts with other packages names.
\item Documentation (\texttt{man/}): It should be available through the \texttt{help()} function and contains the general description and usage of each function of the package.
\item Vignettes (\texttt{vignettes/}): It describes the purpose of the package and how it uses the functions to achieve it. It also contains examples of usage.
\item Sample data (\texttt{data/}): It contains usable data to test the functions and see how it works.
\end{itemize}

The main analysis performed by the \textit{lanalytics} package are represented with plots. As mentioned before, the usability of the package is one of the main purposes, therefore the grammar of graphics will be used to create each one. This framework has many advantages: it builds layered graphics, so the title, annotations, axis, labels, theme and colors are entirely customizable by the user (even after it is created). In \textsf{R}, this can be accomplished with the \texttt{ggplot2} package \cite{ggplot2}.

The user interface will be done as a Shiny app, a web application framework for \textsf{R} \cite{shiny}. Shiny has the big advantage that is written in \textsf{R} code and is open-source, so it will have full compatibility with the complementary \textsf{R}-package. Technically, a Shiny app consist in two files that correspond to the backend and the frontend components. The backend is written in a file called \textit{server.R}, where all the analysis codes will be run, while the frontend is a file called \textit{ui.R} that correspond to the user interface interactive component.

Besides the technical component of the implementation and deployment of the package, its content should be analyzed. As said before, the methods that will be included in the \textsf{R}-package are based on the \textit{lanalytics} package, in addition with additional features that will help in the student's assessment. The functions included in this software package analyze the relationships between variables and variable transformations. This way, relevant analysis about the invested time to answer each item, the total invested time for the test, the order of item answering, the easiness of the questions or even the cognitive level of each item can be made in order to obtain valuable information. Some of the current implemented capabilities are:

\begin{itemize}
\item Compute invested time per question. Computes the time that each student spent in each question.
\item Item Response Time calculation. Finds the median time that students spent to answer an item. 
\item Compute total time invested in the test. 
\item Detect possible incidences of guessing and cheating.
\item Calculates item discrimination.
\item Computes the total scores per student.
\item TEL (Time-Easiness-Level) plots that analyses the Item Response Time with the easiness and cognitive level in one single plot.
\item Order of answering of the items.
\end{itemize}

The \textit{lanalytics} package can be extended for more analysis. Depending on the insights that will be found on the datasets, new features and capabilities will be added. Some other ideas for analysis that can be tried are (among others):

\begin{itemize}
\item Find the relationship between time to deadline and invested time on the quiz
\item Find the relationship between time to deadline and total score per student
\item Analyze the student progression through the semester (The student is performing above or below average?).
\item Track the improvement of items in the quizzes and compare it with the performance of students in different generations.
\item Compare student performance in the low-stakes tests and the student performance in the final high-stakes test. Is there a relevant relationship between them? (if information is available).
\end{itemize}

% ===========================================
% ===========================================
% ===========================================

\section*{Evaluation}
%Evaluation: Details of the metrics by which you will evaluate the outcomes of your research; e.g., by comparing the output of your system with some gold standard, or with the ways in which humans perform a task, etc.
At the end, the package should work with outputs from the learning catalytics software \cite{Schell2013}, also with general text files that contains the same information in a text file. In addition, the codes should manage several date and hour formats. Finally, the documentation, vignettes and Github page should contain clear instructions and the motivations for the package. 

To test the correctness and performance of the \textsf{R}-package and the Shiny-app, they will be tested with two real-world datasets. The first will be a 29 low-stakes open book quizzes from the Molecular and Cell Biology course at Harvard Medical School (2013, 2014), while the second is from the Cell Biology course at Queen Margaret University (2017). The datasets had been anonymized. All the functions will be tested and will be used as an example to produce several synthetic datasets. These will be created to show the functionality and capability of each function, as well as the necessary code for each one. Finally, the shiny-app should have full integrability with the \textsf{R}-package in order that the posterior developments does not need major changes in the appearance of the shiny-app.


% ===========================================
% ===========================================
% ===========================================

\section*{Outputs}
%Outputs: A description of what the outputs of the projects will be: e.g., these might include an extension or change to some existing theory or to some piece of software, some new data (e.g., annotated linguistic data), and so on.

The main outcomes for this project are three:

\begin{itemize}
\item An open-source \textsf{R}-package that will be published with the corresponding vignettes, documentation and synthetic datasets. 
\item A Shiny-app that given a text file as an input, it show the selected graphs analysis.
\item A Github page explaining the uses of this analysis tool and that host the open source repository. 
\end{itemize} 

The proposed technologies and implementations seems suitable for the purpose of the project, as they provide reproducible, usable and easy to download software. This is relevant because it will provide automatic analysis and by doing so it allows to improve the workflow of the analysis tools for educators interested in this topic.

\section*{Work plan}
%Workplan: A timetable or research plan, detailing what will be done to complete the proposed project, and when these tasks will be completed by
\begin{landscape}

\begin{table}[]
\centering
\caption{Work plan for dissertation}
\label{tbl1:workplan}
\begin{tabular}{lll}
\hline
Week                           & Dates                                          & Work to do  \\ \hline
\multicolumn{1}{|l|}{Week 1}   & \multicolumn{1}{l|}{May 26th - June 1st}       & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}- Update codes to latest package versions, consider carefully package dependencies. \\ - Read about license types and data protection law. \\ - Create the general structure of the package. What \textsf{R}-object will be created? \\ - Think about the arguments and outputs of functions. \\ - Think about the general format of the input.\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{Week 2}   & \multicolumn{1}{l|}{June 2nd - June 8th}       & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}- Look for current requirements for package publication and documentation.\\ - Make a draft of the structure of the \textsf{R}-vignettes and the reference manual.\\ - Search for possible conflicts with other name-spaces or packages.\\ - Research about Jekyll technology to add a page to the repository.\end{tabular}}  \\ \hline
\multicolumn{1}{|l|}{Week 3/4} & \multicolumn{1}{l|}{June 9th - June 22th}      & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}- Implement points in the to-do list of the repository.\\ - Generalize methods to work with different years\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{Week 5}   & \multicolumn{1}{l|}{June 23th - June 29th}     & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}- Make first version of package.\\ - Make first version of Github page for the repository.\end{tabular}}\\ \hline \multicolumn{1}{|l|}{Week 6/7} & \multicolumn{1}{l|}{June 30th - July 13th}     & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}- Test usability of the package with friends.\\ - Create synthetic datasets for the package.\\ - Make first draft of \textsf{R}-vignettes and package documentation.\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{Week 8/9} & \multicolumn{1}{l|}{July 14th - July 27th}     & \multicolumn{1}{l|}{- Work on additional features for the package} \\ \hline
\multicolumn{1}{|l|}{Week 10}  & \multicolumn{1}{l|}{July 28th - August 3rd}    & \multicolumn{1}{l|}{- Write dissertation paper} \\ \hline
\multicolumn{1}{|l|}{Week 11}  & \multicolumn{1}{l|}{August 4th - August 10th}  & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}- Final draft of the \textsf{R}-vignettes and package documentation\\ - Final version of Github page for repository and Shiny-app deployment.\\ - Final version of dissertation\end{tabular}} \\ \hline
\multicolumn{1}{|l|}{Week 12}   & \multicolumn{1}{l|}{August 11th - August 18th} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}- Check and test \textsf{R}-package\\ - Check final \textsf{R}-vignettes and package documentation\\ - Check final github page and Shiny-app deployment\\ - Check final dissertation report\end{tabular}} \\ \hline
\end{tabular}
\end{table}

\end{landscape}

\bibliographystyle{apacite}
\bibliography{ref}
\end{document}
