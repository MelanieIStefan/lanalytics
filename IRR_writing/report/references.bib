@phdthesis{gal2016uncertainty,
  title={Uncertainty in Deep Learning},
  author={Gal, Yarin},
  year={2016},
  school={PhD thesis, University of Cambridge}
}

@book{draper1966applied,
  title={Applied regression analysis},
  author={Draper, Norman Richard and Smith, Harry and Pownell, Elizabeth},
  volume={3},
  year={1966},
  publisher={Wiley New York}
}

@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, Christopher JC},
  journal={AT\&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},
  year={2010}
}

@book{neal2012bayesian,
  title={Bayesian learning for neural networks},
  author={Neal, Radford M},
  volume={118},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@inproceedings{salimans2015markov,
  title={Markov chain Monte Carlo and variational inference: Bridging the gap},
  author={Salimans, Tim and Kingma, Diederik P and Welling, Max and others},
  booktitle={International Conference on Machine Learning},
  pages={1218--1226},
  year={2015}
}

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@misc{reuters,
  title = {Tesla working on ’improvements’ to its autopilot radar changes after model s owner became the first selfdriving fatality.},
  author = {AP and REUTERS},
  howpublished = {\url{http://www.dailymail.co.uk/news/article-3693494/Tesla-working-autopilot-radar-changes-self-driving-fatality-men-watching-Harry-Potter-video.html}},
  year={2016}
}

@inproceedings{graves2011practical,
  title={Practical variational inference for neural networks},
  author={Graves, Alex},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2348--2356},
  year={2011}
}

@article{bishop2006pattern,
  title={Pattern recognition},
  author={Bishop, Christopher M},
  journal={Machine Learning},
  volume={128},
  year={2006}
}

@book{bishop1995neural,
  title={Neural networks for pattern recognition},
  author={Bishop, Christopher M},
  year={1995},
  publisher={Oxford university press}
}

@article{bishop1997bayesian,
  title={Bayesian neural networks},
  author={Bishop, Christopher M},
  journal={Journal of the Brazilian Computer Society},
  volume={4},
  number={1},
  year={1997},
  publisher={SciELO Brasil}
}

@book{nocedal2006numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@book{dongarra1998numerical,
  title={Numerical linear algebra for high-performance computers},
  author={Dongarra, Jack J and Duff, Iain S and Sorensen, Danny C and Van der Vorst, Henk A},
  volume={7},
  year={1998},
  publisher={Siam}
}

@inproceedings{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993},
  organization={ACM}
}

@inproceedings{barber1998ensemble,
author = {D. Barber, Christopher Bishop},
title = {Ensemble learning in Bayesian neural networks},
booktitle = {Generalization in Neural Networks and Machine Learning},
year = {1998},
month = {January},
abstract = {

Bayesian treatments of learning in neural networks are typically based either on a local Gaussian approximation to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations. A third approach, called `ensemble learning', was introduced by Hinton (1993). It aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence between the true posterior and a parametric approximating distribution. The original derivation of a deterministic algorithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and hence was unable to capture the posterior correlations between parameters. In this chapter we show how the ensemble learning approach can be extended to full-covariance Gaussian distributions while remaining computationally tractable. We also extend the framework to deal with hyperparameters, leading to a simple re-estimation procedure. One of the benefits of our approach is that it yields a strict lower bound on the marginal likelihood, in contrast to other approximate procedures.


},
publisher = {Springer Verlag},
url = {https://www.microsoft.com/en-us/research/publication/ensemble-learning-in-bayesian-neural-networks/},
address = {},
pages = {215–237},
journal = {},
volume = {},
chapter = {},
isbn = {},
}

@article{buntine1991bayesian,
  title={Bayesian back-propagation},
  author={Buntine, Wray L and Weigend, Andreas S},
  journal={Complex systems},
  volume={5},
  number={6},
  pages={603--643},
  year={1991}
}

@book{gelman2014bayesian,
  title={Bayesian data analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Rubin, Donald B},
  volume={2},
  year={2014},
  publisher={Chapman \& Hall/CRC Boca Raton, FL, USA}
}